{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Gradient Descent\n", "\n", "Gradient descent is an iterative optimization algorithm used to minimize a function by moving in the direction of steepest descent, as defined by the **negative** of the gradient of the function, in steps (i.e. iteratively). \n", "\n", "Recall that the gradient is a fancy word for derivative, or the rate of change of a function. \n", "* The gradient is a vector that points in the direction of greatest increase of a function. \n", "\n", "Think of performing gradient as wanting to get to the bottom of a hill from a starting point away from the bottom of the hill. To get to the bottom of the hill the fastest, you take the direction opposite to the direction of steepest ascent, which we call the direction of steepest _descent_. \n", "\n", "<img src=\"images/mountain.jpeg\" width = 800>\n", "\n", "> Imagine you set a ball rolling down to the bottom of the mountain: the ball will take the path towards the bottom of the hill determined by gradient descent!"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# A simple algorithm for Gradient Descent \n", "\n", "In general, the steps for gradient descent are: \n", "\n", "1. Initialize a random starting point for the parameter(s) you're trying to optimize. \n", "\n", "2. Pick a learning rate. This, along with the gradient (or slope) of the cost function at the starting point, determines the step size. *Be careful not to pick too large of a step size!*\n", "\n", "3. Choose a maximum number of iterations or steps to take. The algorithm will stop after this many steps if a minimum has yet to be found. \n", "    \n", "4. Calculate the gradient at the current point (initially, the starting point)\n", "\n", "5. Take a step in the direction opposite to the direction of the gradient. The step size is controlled by the learning rate and the size of the gradient at the current point. \n", "\n", "6. Repeat the steps until the maximum number of iterations is met. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Notes on the learning rate: [link](https://www.jeremyjordan.me/nn-learning-rate/)\n", "\n", "1. A small learning rate requires many updates before reaching the minimum \n", "2. The optimal learning rate quickly converges to the minimum point \n", "3. A learning rate that is too large leads to divergent behavior: you may bounce around the minimum!  \n", "\n", "<img src=\"images/learning_rates.png\" width = 800>\n", "\n", "_Image from [An introduction to Gradient Descent](https://medium.com/@montjoile/an-introduction-to-gradient-descent-algorithm-34cf3cee752b)_"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's go over gradient descent with a simple example. \n", "\n", "# Gradient descent in 1-d"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Find the minimum of $ y = (x + 5)^2 $ starting from $x = 3$ using gradient descent. \n", "\n", "* Set the learning rate to 0.01. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's visualize the function before applying gradient descent. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np\n", "\n", "import matplotlib.pyplot as plt \n", "%matplotlib inline\n", "\n", "np.random.seed(42)\n", "\n", "x = np.linspace(-15, 10, 500)\n", "y = (x + 5)**2\n", "\n", "plt.plot(x, y)\n", "plt.xlabel('x')\n", "plt.ylabel('f(x)')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What happens to the slope of the lines tangential to a function close to the function's minimum? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What can we expect will happen to the step size in the gradient descent algorithm as we reach the minimum? \n", "\n", "_Remember that the step size is controlled by the learning rate and the size of the gradient or rate of change of the function at the current point._"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's use the technique of gradient descent to find the minimum of this function. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# This is our starting point \n", "current_x = 3 \n", "\n", "# This is our learning rate \n", "learning_rate = 0.01 \n", "\n", "# Number of iterations \n", "n_iterations = 10000\n", "\n", "# The gradient of a function of one variable is its derivative! \n", "df = lambda x: 2*(x+5)\n", " \n", "for i in range(n_iterations):\n", "    \n", "    # store the current value of x in a new variable \n", "    previous_x = current_x\n", "    \n", "    # update the value of x: \n", "    current_x = current_x - learning_rate * df(previous_x)\n", "    \n", "    print('iteration', i+1, \"\\nX value is\", cur_x)\n", "\n", "print(\"Minimum occurs at\", current_x)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Does this match what we expect from taking the derivative of f(x) and setting it equal to zero? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Gradient descent: Application to Linear Regression"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, let's apply gradient descent to a more complicated problem: finding the line of best fit in a simple linear regression problem. \n", "\n", "Remember that linear regression is a linear approach to modeling the relationship between a dependent variable and one or more independent variables. \n", "\n", "In the case of **simple** linear regression, where we have one independent variable x, we seek __to find the line of best fit to our data__, defined by:\n", "\n", "$$ y = mx + b $$\n", "\n", "Here, m is the slope of the line and b is the y-intercept. \n", "\n", "Our task is to find the values of m and b that give the minimum error with respect to the data.  "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's bring in some sample data: "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Set a random seed for reproducibility\n", "np.random.seed(42) \n", "\n", "x = 2 * np.random.rand(100,1)\n", "y = 4 + 3*x + np.random.randn(100,1)\n", "\n", "plt.scatter(x, y)\n", "plt.xlabel('x')\n", "plt.ylabel('y')"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We seek to find the __line of best fit__ through this data."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Cost function for linear regression\n", "\n", "The cost or loss function is the error in our prediction. We use the mean squared error to compute the cost. \n", "\n", "Remember that for a line with slope m and y-intercept b, the mean squared error is given by: \n", "\n", "$$ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left( y_i - \\left(mx_i + b\\right)\\right)^2 $$\n", "\n", "where, for each of the n values of x, $x_i$, we have found the difference between the actual value of y, $y_i$, and the predicted value of $y_i$, squared the difference, and found the mean of the squared difference for every value in x. \n", "\n", "The cost function $J$, which is equal to the mean squared error, is then:  \n", "\n", "$$ J(m, b) = \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left( y_i - \\left(mx_i + b\\right)\\right)^2 $$\n", "\n", "We seek to find values of m and b that minimize the cost $J(m, b)$ given the data. \n", "\n", "**We will find the optimal values of m and b using gradient descent.**"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Gradient descent applied to simple linear regression \n", "\n", "Here are the steps to follow and which you'll implement below: \n", "\n", "1. Start with random values for m and b. \n", "2. Set the learning rate L. (Remember that the learning rate controls how much the values of the parameters change in each iteration of the algorithm). \n", "3. Set a maximum number of iterations. \n", "\n", "4. Calculate the partial derivative of the loss function with respect to m, $\\partial J/\\partial m$, and plug in the current values of m and b to find the value of $\\partial J/\\partial m$ at the starting point. \n", "\n", "5. Similarly, calculate the partial derivative of the loss function with respect to b, $\\partial J/\\partial b$, and plug in the current values of m and b to find the value of $\\partial J/\\partial b$ at the starting point. \n", "\n", "6. Update the current values of m and b using the following update rules and the values of the partial derivatives you have just computed:\n", "\n", "$$ m = m - L  \\frac{\\partial J}{\\partial m} $$\n", "\n", "\n", "$$ b = b - L  \\frac{\\partial J}{\\partial b} $$\n", "\n", "7. Repeat steps 4 through 6 until all iterations are completed. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Before getting started, we provide you with the partial derivatives of the cost function J with respect to m and b: \n", "\n", "$$ \\frac{\\partial J}{\\partial m} = -\\frac{2}{n}\\sum_{i=1}^{n}\\left( y_i - \\left(mx_i + b\\right)\\right)x_i $$\n", "\n", "\n", "$$ \\frac{\\partial J}{\\partial b} = -\\frac{2}{n}\\sum_{i=1}^{n}\\left( y_i - \\left(mx_i + b\\right)\\right)$$"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The equation for the cost function for line with slope m and y-intercept b is \n", "\n", "$$ J(m, b) = \\frac{1}{n}\\sum_{i=1}^{n}\\left( y_i - \\left(mx_i + b\\right)\\right)^2 $$\n", "\n", "\n", "Assuming `x` and `y` are numpy arrays, how would you write this expression in Python? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's use gradient descent to find the optimal values for m and b for our example. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# Start with random values for m and b\n", "m, b = np.random.rand(None)\n", "\n", "# Set the learning rate to 0.001\n", "L = None \n", "\n", "# Set the number of iterations to 10000\n", "n_iterations = None \n", "\n", "# Keep track of the cost after each iteration\n", "costs = []\n", "\n", "for i in range(None):\n", "    \n", "    # What are the current predicted values of y given\n", "    # the current m and b values?\n", "    y_pred = None \n", "    \n", "    # What is the length of y_pred? \n", "    n = None \n", "    \n", "    # What is the error (or cost) of the prediction?\n", "    cost = None \n", "    \n", "    # Add the cost to the list of costs after each iteration\n", "    costs.append(cost)\n", "    \n", "    # Compute the partial derivative of the cost with respect to m and b\n", "    # at the current values of m and b\n", "    partial_cost_wrt_m = None \n", "    partial_cost_wrt_b = None \n", "    \n", "    # Update the values of the parameters m and b.\n", "    m = m - None  \n", "    b = b - None \n", "    "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What values did you obtain for m and b? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Plot the line with the parameters m and b alongside the data. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a plot of the cost vs the number of iterations. What do you observe happens to the cost as the number of iterations increases?"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Sigmoid Function "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's switch gears and discuss the sigmoid function. \n", "\n", "The sigmoid function f(z) is defined as: \n", "\n", "$$ \\large f(z) = \\frac{1}{1 + e^{-z}} $$"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a plot of the sigmoid function for x values ranging from -10 to 10. \n", "\n", "Set `z = np.linspace(-10, 10, 10000)`. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What are the maximum and minimum values of the sigmoid function? \n", "\n", "What is the value of the sigmoid function when performing logistic regression for classification? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Summary: \n", "\n", "1. Gradient descent is an iterative algorithm used to find the minimum of a function. \n", "2. Gradient descent can be used in linear regression, where it is used to find the best parameters that minimize the mean squared error.\n", "3. The sigmoid function maps numbers ranging from negative infinity to infinity to the range from 0 to 1. "]}], "metadata": {"celltoolbar": "Slideshow", "kernelspec": {"display_name": "learn-env", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}